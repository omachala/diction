# Diction — Self-Hosted Whisper Transcription
#
# Pick one model and start it:
#
#   docker compose up -d whisper-small        # recommended default
#   docker compose up -d whisper-tiny         # fastest, lowest RAM
#   docker compose up -d whisper-medium       # better with accents/noise
#   docker compose up -d whisper-large        # best accuracy
#   docker compose up -d whisper-distil-large # near-best accuracy, 6x faster
#
# Or run multiple models at once:
#   docker compose up -d whisper-small whisper-medium
#
# Then point the Diction app endpoint to:
#   http://<your-server-ip>:9002   (port depends on model, see below)
#
# Models, ports, and memory requirements:
#
#   Model          Port   RAM        Latency (CPU)
#   ─────────────────────────────────────────────────
#   tiny           9001   ~350 MB    ~1-2s
#   small          9002   ~800 MB    ~3-4s
#   medium         9003   ~1.8 GB    ~8-12s
#   large-v3       9004   ~3.5 GB    ~20-30s
#   distil-large   9005   ~2.0 GB    ~4-6s
#
# Models are downloaded on first start and cached in a shared volume.
# Subsequent starts are instant.

services:

  whisper-tiny:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: diction-whisper-tiny
    restart: unless-stopped
    ports:
      - "9001:8000"
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      WHISPER__MODEL: Systran/faster-whisper-tiny
      WHISPER__INFERENCE_DEVICE: cpu
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s

  whisper-small:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: diction-whisper-small
    restart: unless-stopped
    ports:
      - "9002:8000"
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      WHISPER__MODEL: Systran/faster-whisper-small
      WHISPER__INFERENCE_DEVICE: cpu
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s

  whisper-medium:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: diction-whisper-medium
    restart: unless-stopped
    ports:
      - "9003:8000"
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      WHISPER__MODEL: Systran/faster-whisper-medium
      WHISPER__INFERENCE_DEVICE: cpu
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s

  whisper-large:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: diction-whisper-large
    restart: unless-stopped
    ports:
      - "9004:8000"
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      WHISPER__MODEL: Systran/faster-whisper-large-v3
      WHISPER__INFERENCE_DEVICE: cpu
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s

  whisper-distil-large:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: diction-whisper-distil-large
    restart: unless-stopped
    ports:
      - "9005:8000"
    volumes:
      - whisper-models:/root/.cache/huggingface
    environment:
      WHISPER__MODEL: Systran/faster-distil-whisper-large-v3
      WHISPER__INFERENCE_DEVICE: cpu
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 120s

volumes:
  whisper-models:
    # Shared across all models — downloaded once, reused by all
    name: diction-whisper-models
